Better align page allocation methord for arenas. request arenasize -1 cell to leave space for book keeping of dl malloc 

Add per Lua state allocators maybe set them to global allocators when entering the jit and restore the oldones when exiting the jit

a special preallocate jit only api call to pass in a list of sizes of objects tobe allocated for the trace and a pointer to a buffer a that gets gets filled in with a list of cellids. See independent_calloc from 
ptmalloc

Could store arena grey queue pointers negative of tha arena address some 64 byte multiple based on hased upper bits of the arena address to avoid cache aliasing at the expense more complex address calculations

How to handle losing 2 whites which allowed allocating and sweeping at the same time but avoiding sweeping new objects 
 every object allocated while the gc is in either sweep phase must be allocated black or otherwise every sweep step would need to walk the stack and mark everything
 but this not allow minor collections or would ruin there effectivness
 

Custom arenas with a user provided callback function to call for every object sweape/dead instead of finalizers, fixed object size to translate object address/cellid into ids

Could allocate mcarenas inside a GCArena to allow gc'able intrinsic code and the cdata and code is treated as one object so when the cdata for the intrinsic is dead the code is as well

Could allocate arena object memory as uncommited to start with until its used. Could use a fast check where when allocating from an arena check if the new cell top passed a 4k boundry

Add a special arena mode to allocate a cell of metadata behind every object sweep functions can just bit shift block right 1 to also mark the metadata
  Value of the GC Cycle counter
  Maybe Time stamp
  func proto of where the allocation happened


How to allocated aligned pages for an arena on Linux without massive over allocation since we don't get the same allignment garantees as windows with VirtualAlloc

alot of the bit primitives used are duplicates of ones used for register sets of the JIT maybe build a shared set

The global allocated string table should be switched from chained to open addressing to play nice with the new arenas and avoid having load the memory of dead strings just to 
follow the next string in a linked list

Could bitwise and the pointer of the graylist pointer with some -1 of power of 2 to check if it needs reallocating avoiding the need to also have the greybase pointer before 
the block words. The allocated size of the list could be negative of the greybase pointer


How should the grey list work when stuff refs other stuff in the same gc arena 

Done:
  
An idea to handle avoid trying to mark GG_State.L or global_State.strempty that are not in arena would be allocating GG_State aligned to the same alignment as arenas which 
will cause the computed a cell id from there addresses tobe an id less than MinCellId that we can check for. Maybe this can just be turned into a mask check
OR allocate GG inside an arena and we always mark them black in the arena when a GC starts. L/emptystring will need tobe 16 byte aligned in GG so we don't trigger asserts
Could also always use a huge page for this arena using the extra space of the page for other arenas.

How should freeing tables be handled since unlike most other objects it has internal memory pointers
  Table memory could be allocated from the arena. When we turn the table grey we also turn the table array and hash pointers black if there memory address is inside the memory range of the arena(could also check 16 byte aligned).
  Could have a bitmap of which block words contain table objects. Only clear the bit if we find no tables alive in a block word.
  
Could pointers for huge gc objects not in an arena be aligned on a non 16 byte boundry for fast way to skip trying access there arena metadata
Overview implys the base adddres of huge objects is aligned to the same as an arena so its masked cell id will be 0?

How to handle aligned\varible length cdata  the gc header will be negative of the pointer causeing the incorrect cell tobe marked turning it from an extent.
 Allowed cdata to start 8 bytes into a cell so the data is aligned by 16 bytes in the next cell higher alignment than 16 bytes can be done at the cell level
 
 How should __gc for cdata be handled
Reddit thread comment says in a unrolled linked list(of cell ids?) remarked grey so there not freeded before the gc runs